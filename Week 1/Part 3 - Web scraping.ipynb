{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Francisco Pereira [camara@dtu.dk], DTU Management*\n",
    "\n",
    "*Stanislav Borysov [stabo@dtu.dk], DTU Management*\n",
    "# Advanced Business Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 1 - Web Data Mining - Part 3: Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web crawling and scraping represent a very flexible way to get the content from the Internet. Essentially, it imitates a user who visits different webpages and views their content. The only difference is that Internet companies usually love real users and hate scraping bots. So be prepared to be blocked. **In the worst case, you can get into serious troubles so please always read Terms & Conditions and follow the company's policy about automatic data collection (or ask them directly if you are not sure)!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will implement our web scraper. However, if you want to do it properly, just use one of the many frameworks available, for example, Scrapy (https://scrapy.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to get some data from the DTU website. Particularly, we want to get news headlines together with the date and short description from https://www.dtu.dk/english/news. Hopefully, we will not get banned or sued."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Navigating the website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the webpage shows 20 recent news items. But you can automatically navigate through the list of news in to whatever order you prefer e.g. get the news items from 25 to 45. Or even more interesting, you can ask for a specific time window. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fr=FIRST_ITEM`- From Nth item, where 0 is the most recent one\n",
    "\n",
    "`fd=DD-MM-YYYY`- From date\n",
    "\n",
    "`td=DD-MM-YYYY`- To date\n",
    "\n",
    "For example, https://www.dtu.dk/english/news?fd=05-01-2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Tip: you can find yourself which other parameters you can use, besides \"fr\", \"fd\" and \"td\", by manually using the website... ;-) for example, can you figure out to search for specific string in the news feed? ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = \"https://www.dtu.dk/english/news\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and print a list of URLs to get webpages which contain the last 200 news items. \n",
    "\n",
    "***WARNING! Just generate the list and do not access the content!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.dtu.dk/english/news?fr=1', 'https://www.dtu.dk/english/news?fr=21', 'https://www.dtu.dk/english/news?fr=41', 'https://www.dtu.dk/english/news?fr=61', 'https://www.dtu.dk/english/news?fr=81', 'https://www.dtu.dk/english/news?fr=101', 'https://www.dtu.dk/english/news?fr=121', 'https://www.dtu.dk/english/news?fr=141', 'https://www.dtu.dk/english/news?fr=161', 'https://www.dtu.dk/english/news?fr=181']\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "urls = []\n",
    "n_items = 200\n",
    "items_per_page = 20\n",
    "start = 1\n",
    "\n",
    "for i in range(math.ceil(n_items / items_per_page)):\n",
    "    parameters = [\n",
    "        \"fr={}\".format(start)\n",
    "    ]\n",
    "    url = url_base + \"?\" + \"&\".join(parameters)\n",
    "    urls.append(url)\n",
    "    start += items_per_page\n",
    "\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scraping the webpage content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***We will not do it.*** I already scrapped a webpage with the first 20 news items and saved it using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import urllib.request\n",
    "url = \"https://www.dtu.dk/english/news?fr=0\"\n",
    "\n",
    "contents = urllib.request.urlopen(url).read()\n",
    "with open(\"dtu_news.html\", \"wb\") as f:\n",
    "    f.write(contents)\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that all markup and images are missing since we saved only the HTML content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extracting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dtu_news.html\", \"r\") as f:\n",
    "    contents = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task to extract the news items from the contents using either regular expressions or HTML parser (e.g., PyQuery, BeautifulSoup) and save them to a JSON file.\n",
    "\n",
    "*HINT: To better understand the structure of the webpage, try:*\n",
    "- Chrome: right click -> show source\n",
    "- Safari, Firefox, Edge: right click -> inspect element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Our goal is to end up with an output that should look like the following\n",
    "news = [\n",
    "    ...,\n",
    "    {\n",
    "        'url': 'https://www.dtu.dk/english/news/Nyhed?id={B4C5541F-8E2F-4118-9DA5-07A2BA6E2407}',\n",
    "        'title': 'New study underlines sea level rise in the Arctic ocean',\n",
    "        'desc': 'Sea levels in the Arctic oceans have risen an average of 2.2 millimeters per year over the last 22 years.This is the conclusion reached by a Danish-German research team...',\n",
    "        'date': '16 JUL'\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "\"\"\";\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: the PyQuery package can help you directly extract content from HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyquery import PyQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq = PyQuery(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div.newsItem>, <div.newsItem>, <div.newsItem>, <div.newsItem>, <div.newsItem>, <div.newsItem>, <div.newsItem>, <div.newsItem>, <div.newsItem>, <div.newsItem>, <div.newsItem>, <div.newsItem>, <div.newsItem>, <div.newsItem>, <div.newsItem>, <div.newsItem>, <div.newsItem>, <div.newsItem>, <div.newsItem>, <div.newsItem>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pq('div.newsItem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsItems.eq(2).text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': None, 'title': '', 'desc': '', 'date': ''}\n"
     ]
    }
   ],
   "source": [
    "news = []\n",
    "\n",
    "#Loops through the elments in newsItems, scrapping the info needed adding it to the array news \n",
    "for i in range(len(newsItems)):\n",
    "    item = newsItems.eq(i)\n",
    "    # url\n",
    "    item_url = item.find('h2>a').attr('href')\n",
    "    # title\n",
    "    item_title = item.find('h2>a').text()\n",
    "    # desc\n",
    "    item_desc = item.find('p').text()\n",
    "    # date\n",
    "    item_date = item.find('span.date').text()\n",
    "    # \n",
    "    new_item = {\n",
    "        'url': item_url,\n",
    "        'title': item_title,\n",
    "        'desc': item_desc,\n",
    "        'date': item_date\n",
    "    }\n",
    "    print(new_item)\n",
    "    news.append(new_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
