{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings!\n",
    "\n",
    "In this notebook, we are going to work with word embeddings. Word embeddings are vectors that represent words in a specific language in an N-dimensional space, such as 200, 300 or even more.\n",
    "\n",
    "Let's import some necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import normalize\n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: adjustText in c:\\users\\ioaar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (0.7.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ioaar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from adjustText) (3.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ioaar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from adjustText) (1.21.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ioaar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from matplotlib->adjustText) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ioaar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from matplotlib->adjustText) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\ioaar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from matplotlib->adjustText) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\ioaar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from matplotlib->adjustText) (2.8.0)\n",
      "Requirement already satisfied: six in c:\\users\\ioaar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->adjustText) (1.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ioaar\\appdata\\roaming\\python\\python37\\site-packages (from kiwisolver>=1.0.1->matplotlib->adjustText) (60.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\ioaar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\ioaar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\ioaar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\ioaar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\ioaar\\AppData\\Local\\Continuum\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Install the adjustText package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install adjustText\n",
    "from adjustText import adjust_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of pretrained embeddings are offered online in different languages. For example, you can download word embeddings trained on raw Wikipedia text from here:\n",
    "\n",
    "https://github.com/facebookresearch/fastText/blob/master/docs/pretrained-vectors.md\n",
    "\n",
    "These vectors have 300 dimensions and were obtained using using fastText and the skip-gram model described in Bojanowski et al. (2016) with default parameters.\n",
    "\n",
    "You should download the embeddings in **text** format. Downloading these files may take a while as they are several gigabytes in size, depending on the language you choose.\n",
    "For the purposes of this notebook, you can use a shortened version of word embeddings in english: ```wiki.en.vec.short``` , that you can find in the ``` Notebooks``` folder.\n",
    "\n",
    "The files follow a specific format.\n",
    "\n",
    "##### FILE FORMAT:\n",
    "\n",
    "The **first line** in the file contains two numbers separated by a single space. The first number indicates the number of words in the file (`N_WORDS`) and the second number specifies the number of dimensions (`N_DIMENSIONS`) that are used to represent those words.\n",
    "\n",
    "After the first line, each line will contain one word at the beginning. Following the word, and separated by spaces, there will be `N_DIMENSIONS` numbers, which represent each word in the space.\n",
    "\n",
    "The words are sorted by their frequency in the wikipedia corpus. The first words in the file will be the most frequent ones. Let's read and print the first 4 lines of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['299999 300\\n',\n",
       " ', -0.023167 -0.0042483 -0.10572 0.042783 -0.14316 -0.078954 0.078187 -0.19454 0.022303 0.31207 0.057462 -0.11589 0.096633 -0.093229 -0.034229 -0.14652 -0.11094 -0.11102 0.067728 0.10023 -0.067413 0.23761 -0.13105 -0.0083979 -0.10593 0.24526 0.065903 -0.2374 -0.10758 0.0057082 -0.081413 0.26264 -0.052461 0.20306 0.05062 -0.18866 -0.11494 -0.25752 0.046799 -0.050525 0.06265 0.15433 -0.056289 -0.048437 -0.099688 -0.035332 -0.091647 -0.081151 -0.0010844 -0.08414 -0.13026 0.01498 -0.086276 -0.053041 -0.10644 -0.042314 0.086469 0.22614 -0.16078 0.18845 0.053098 -0.21475 0.16699 -0.14442 -0.1593 0.0062456 -0.07663 -0.091568 -0.28984 0.027078 0.021275 0.023939 0.14903 -0.33062 -0.097811 -0.033814 0.070587 0.023294 0.065382 0.18716 -0.13444 0.14431 -0.0268 -0.022903 0.097554 -0.032909 -0.027827 -0.068771 0.17053 -0.05946 0.020424 -0.077589 0.1216 -0.077437 0.10665 0.051087 0.0076379 -0.064936 0.09031 0.059447 0.0048881 0.078309 -0.012163 0.062155 -0.072664 0.17857 -0.22874 0.066397 -0.039295 -0.027717 0.061571 0.072824 -0.092512 -0.087984 -0.12753 -0.0018705 0.18689 0.0051173 -0.0013532 0.043246 0.10867 -0.12209 -0.0091676 0.23938 -0.059501 -0.0010456 0.086584 0.020238 0.21686 0.16495 0.037256 0.12343 0.17706 0.075777 0.031022 -0.12948 0.030936 0.096897 -0.10793 0.12644 -0.056489 0.082232 0.20679 0.11679 0.13965 0.26362 0.037603 -0.003105 -0.089501 -0.0076969 -0.11654 -0.28567 0.046616 -0.0082062 0.15621 -0.14641 0.064561 -0.1133 0.27129 0.14532 -0.021773 0.23305 -0.1617 0.15705 0.13845 0.022417 -0.10982 -0.049431 0.076855 -0.0453 -0.19029 0.011183 -0.010393 0.0016916 0.089407 -0.051022 -0.086066 0.083933 -0.0081962 -0.0077321 0.033991 -0.20092 0.03328 0.062224 0.016121 0.27143 -0.19754 -0.15222 -0.015345 -0.063907 -0.098597 -0.20162 0.14004 -1.1533e-05 -0.18928 0.12253 -0.0070378 0.0864 -0.30255 -0.03908 0.045517 -0.16449 -0.23548 0.052781 0.13847 -0.20022 -0.015974 0.027137 0.18287 -0.02389 0.22072 -0.04271 -0.075939 -0.087386 -0.049337 0.047824 -0.059078 -0.15181 -0.21229 -0.054944 -0.011453 -0.11996 -0.15307 -0.054828 -0.053217 -0.048546 0.028856 -0.094537 0.27144 0.054638 0.059727 0.061772 0.009259 -0.12032 -0.16646 -0.029087 0.0028752 -0.16076 -0.1371 -0.18988 0.022857 0.18455 -0.018236 -0.0060562 0.14302 0.032535 0.14333 -0.030871 -0.15218 0.092813 0.066358 0.018316 -0.24143 0.0054391 -0.064479 -0.08596 0.030446 0.082157 0.026093 0.058985 0.0051085 0.089127 -0.018164 -0.077821 0.0034232 0.13892 0.046106 -0.05417 0.0084399 -0.15362 -0.14735 0.065191 -0.022883 -0.14498 -0.16917 -0.19215 0.10611 0.001678 -0.16331 -0.07307 0.11576 0.083567 -0.060317 -0.064714 0.15305 -0.11949 0.16684 0.14109 0.046036 -0.060393 0.046595 -0.11558 0.044184 -0.023124 0.02586 -0.11653 0.010936 0.089398 -0.0159 0.14866 \\n',\n",
       " '. -0.11112 -0.0013859 -0.1778 0.064508 -0.24037 0.031087 -0.030144 -0.36883 -0.043855 0.24831 0.078633 -0.16072 0.10528 -0.09622 -0.077742 -0.28262 -0.13013 0.0056083 0.19406 0.19287 -0.10955 0.23008 -0.19911 0.18836 -0.21861 0.0017505 -0.093017 -0.25064 0.034323 0.17047 -0.094408 0.1442 -0.18533 0.1685 -0.16582 -0.061712 -0.010346 -0.078817 0.20596 -0.12286 -0.064035 -0.040983 -0.21272 0.097636 -0.058569 -0.044869 0.028206 -0.12619 0.012063 -0.19177 -0.044824 -0.081765 -0.073723 0.037251 -0.075625 0.0085609 0.062612 0.22144 -0.10765 0.1617 0.065163 -0.38164 0.40246 -0.40637 -0.15695 -0.026063 0.08266 -0.21487 -0.19077 0.015702 0.0019313 0.028565 -0.11221 -0.32311 -0.11034 -0.0036508 0.098677 0.044877 -0.037888 0.055357 -0.083052 0.1388 -0.014431 0.03675 0.024439 -0.23493 -0.027343 0.079749 0.033782 -0.14431 -0.12995 0.0013884 0.12936 -0.025757 0.14235 -0.039069 0.04441 -0.10727 0.010207 -0.25722 0.0108 0.13833 -0.12007 0.1007 -0.032493 0.039002 -0.22527 -0.0082154 0.089996 0.12834 -0.042059 0.034393 -0.062866 -0.17397 -0.12022 0.10377 0.23975 0.0168 0.013164 0.1788 0.15969 -0.09956 -0.00033453 0.14833 0.0023907 0.012985 0.018713 -0.013954 0.017909 0.18023 -0.061768 0.13785 0.11028 0.06795 -0.093466 0.065266 0.081291 0.074469 -0.14988 0.27212 -0.24042 0.10926 0.18611 0.25639 0.26599 -0.033769 0.035854 0.046225 -0.096937 -0.022181 -0.0093109 -0.17216 -0.0052176 -0.087209 0.16378 -0.1748 0.10675 -0.11914 0.11579 0.28104 0.031467 0.089572 -0.076408 0.091939 0.10972 0.10825 0.044846 -0.10262 0.011195 0.12233 -0.21444 -0.0085801 -0.20019 0.076398 0.069234 -0.00027189 -0.14671 0.0020823 0.10946 -0.026161 -0.03566 -0.20662 -0.017169 0.14491 -0.018133 0.26558 -0.071692 -0.32038 0.090198 0.038957 -0.13264 -0.082944 0.28167 0.095751 -0.21677 0.036769 0.002427 0.073605 -0.34857 -0.023093 -0.082191 -0.10558 0.029198 0.092549 0.26644 -0.1304 -0.14764 0.2099 0.17682 -0.078109 0.15883 -0.12386 -0.036651 -0.035916 -0.080906 -0.052342 -0.043183 -0.15264 -0.12676 0.13355 0.04599 -0.063914 0.12744 -0.1052 -0.12274 0.08151 0.055437 -0.1856 0.10727 0.098527 0.06622 0.22512 -0.19452 -0.058842 0.060798 0.08935 -0.050251 -0.075885 -0.13478 -0.19397 0.00089476 0.24309 -0.042496 0.016901 0.15118 0.037475 0.13201 0.052854 -0.15011 0.025235 0.02491 0.04034 -0.40257 0.03001 0.014418 -0.0089364 0.036583 0.022269 -0.074602 0.10987 0.018079 0.19326 0.038292 0.080146 0.053901 0.13042 0.004338 -0.15412 0.092865 -0.059374 -0.033619 0.044073 0.066178 -0.16668 -0.062453 -0.2339 -0.026025 -0.028359 -0.25575 -0.10586 0.099423 0.15685 -0.12659 -0.22975 0.15002 -0.16253 0.095208 0.30722 0.052365 -0.10963 0.095332 -0.21914 -0.04276 -0.13685 0.09747 -0.21818 -0.058233 0.063374 -0.12161 0.039339 \\n',\n",
       " 'the -0.065334 -0.093031 -0.017571 0.20007 0.029521 -0.03992 -0.16328 -0.072946 0.089604 0.080907 -0.040032 -0.23624 0.1825 -0.061241 -0.064386 -0.075258 -0.050076 -0.020001 0.003496 0.14487 -0.16791 0.076852 -0.22977 -0.057937 -0.13408 -0.073586 -0.0012575 0.019708 0.056866 0.0625 -0.15555 0.15207 -0.10629 0.2467 -0.027853 -0.17703 0.0072058 -0.11941 0.083843 -0.11843 0.053612 -0.0023144 -0.084279 0.02842 0.078184 -0.12017 -0.040866 0.089438 0.050845 -0.06372 0.070864 -0.063962 -0.095329 0.069848 -0.050254 0.058265 0.085877 0.043966 -0.051179 0.097819 -0.050705 -0.18195 0.32365 -0.076363 0.046492 -0.19886 -0.24429 -0.18651 -0.22465 0.069392 -0.37377 -0.082351 0.061531 -0.13149 -0.075824 -0.060647 0.072747 0.24397 0.021046 -0.071253 0.11115 0.073137 -0.086065 0.11181 -0.0062127 -0.16714 -0.065522 0.083572 -0.092857 -0.12377 -0.082908 0.012025 0.33836 -0.27124 0.054494 -0.088206 0.073294 0.024418 0.0036174 -0.027804 -0.12583 -0.032364 -0.0017323 -0.075066 -0.20324 -0.11735 0.0076592 0.021895 -0.013652 0.064288 -0.0086384 -0.08287 -0.10197 -0.13569 0.085786 -0.0061483 0.15858 0.18609 0.11262 0.090442 0.27457 0.22795 -0.076096 0.21347 0.026208 0.070195 0.12838 0.20542 0.092349 0.12774 -0.17516 0.089942 -0.024982 0.033565 -0.12136 0.059703 -0.060016 0.13908 -0.05639 0.15073 0.095501 0.055378 0.051278 0.037113 0.017116 0.22476 0.046822 0.035514 0.065785 0.094907 0.13325 -0.071157 -0.07789 -0.067566 -0.0713 -0.070124 0.03169 -0.059157 0.14293 0.060211 -0.12124 0.14737 -0.069322 0.084458 0.15567 0.024013 -0.11073 0.075851 0.16277 -0.085473 -0.19668 -0.077685 -0.067194 -0.07725 -0.10461 0.12912 0.0099595 0.24678 -0.0021382 -0.026336 0.045182 0.027762 -0.048843 0.10953 -0.032948 0.24045 0.18437 -0.15205 0.11378 0.04739 -0.0017306 -0.16477 0.19182 0.17582 -0.043354 0.048313 -0.054663 0.026949 -0.17522 0.1202 -0.014748 -0.046279 -0.044321 0.1132 -0.013909 -0.16025 -0.023832 0.011909 0.044407 0.10252 0.2366 0.1022 -0.0089182 -0.053413 0.066031 -0.10562 0.099354 -0.2636 -0.13579 0.029793 -0.0049288 -0.032903 -0.04859 0.14609 -0.033257 0.074076 -0.18396 0.039731 0.00357 -0.029935 0.13167 0.18358 -0.12839 -0.027227 -0.055763 -0.015632 0.018481 -0.096078 0.010891 -0.030878 -0.082804 -0.035578 0.17599 0.044577 0.20949 0.20219 0.11987 -0.12282 0.13082 -0.047396 0.05179 0.24328 0.025997 -0.017327 -0.17035 0.22185 -0.068049 -0.11139 0.033333 -0.12122 0.025779 0.15938 -0.036345 0.0091971 -0.11033 -0.079248 -0.10993 0.085555 0.009754 -0.23608 -0.18619 -0.071835 -0.024813 0.074658 -0.028669 0.031546 0.088931 0.23872 0.168 -0.058967 0.063124 -0.057813 0.019214 0.016109 0.11406 -0.074514 0.051821 0.20783 -0.086803 0.10953 0.064944 -0.21673 -0.037683 0.08186 -0.039891 -0.051334 -0.10165 0.16642 -0.13079 0.035397 \\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=open(\"wiki.en.vec.short\", encoding=\"utf-8\")\n",
    "lines=f.readlines()[:4]\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the file contains embedding vectors of 299999 words and 300 dimensions, while the 3 most frequent words in the wikipedia corpus are ```,```, ```.``` and ```the```. Note that all the information in the file (even the numeric values) is saved as stings. Also note that in the end of each line there is the newline character ```\\n``` that we will have to ignore when storing the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Exercise 1:\n",
    " \n",
    "In order to play with word embeddings, we need a way of storing them. \n",
    "\n",
    "Write a function that reads the file and stores the words and their embeddings in a dictionary.\n",
    "Each word should be mapped to a numpy vector array. Name the dictionary `embs_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_dict= "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are trained based on the distributional properties and contexual information of the input words. For this reason they are able capture semantic associations between them!\n",
    "\n",
    "What does this mean?\n",
    "    \n",
    "> *Similar words tend to have similar vectors*\n",
    "\n",
    "> *Semantically close words occupy neighboring positions in the embedding space.*Â \n",
    "\n",
    "Now that you have saved the words and their corresponding embedding vectors in a dictionary, let's choose some words and  visualize them to see how they are positioned in the embedding space. In order to do this we have to project the 300-dimensional vectors in to 2-D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Exercise 2:\n",
    " \n",
    ">Get the embedding vectors of the following words:\n",
    "\n",
    "```'red'```,```'blue'```, ```'yellow'```, ```'green'```, ```'cat'```, ```'dog'```, ```'pet'```, ```'rabbit'```, ```'gin'```, ```'vodka'```,```'whiskey'```, ```'beer'```, ```'math'```, ```'physics'```, ```'science'```, ```'biology'```\n",
    "\n",
    "and \n",
    ">Plot them in 2-D using [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)   ```fit_transform``` method\n",
    "\n",
    "*Remember to [add annotations](https://adjusttext.readthedocs.io/en/latest/Examples.html) for each point in the plot so that we can see which word corresponds to which point in the space*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=['red','blue', 'yellow', 'green', 'cat', 'dog', 'pet', 'rabbit', 'gin',\n",
    "       'vodka','whiskey', 'beer','math', 'physics', 'science', 'biology']\n",
    "\n",
    "#Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3: \n",
    "\n",
    "Now repeat the same process but instead of using PCA, use [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) for projecting the embedding vectors in 2-D.\n",
    "\n",
    "t-SNE is the most commonly used algorithm for visualizing word embeddings as, compared to other dimensionality reduction methods, it better preserves the global structure of the data!\n",
    "\n",
    "Given that we only considered a very small number of words, we should adjust ```perplexity``` parameter of ```TSNE``` algorithm accordingly to get meaningful results. The default value is ```perplexity= 30``` which makes sense for much bigger datasets. Use a smaller value instead like ```5```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=['red','blue', 'yellow', 'green', 'cat', 'dog', 'pet', 'rabbit', 'gin',\n",
    "       'vodka','whiskey', 'beer','math', 'physics', 'science', 'biology']\n",
    "\n",
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you do it? \n",
    "\n",
    "Good! Now run the previous cell again. What do you observe?\n",
    "\n",
    ">t-SNE is a stochastic algorithm which means that if you run it multiple times on the same data, you will get different results, resulting in different plots. \n",
    "\n",
    ">Does it matter? Not really, because we are interested in the *relative positions between the words* in the space which remain the same.\n",
    "\n",
    ">However, if you want to get the exact same results every time you run the code, you can specify a seed value for `TSNE`  using the `random_state parameter`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity between words"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAATEAAADOCAYAAABW1jUQAAAgAElEQVR4Ae2deZgU1b3+54/73EuSqwGDaBQNuWqMUa9rvGgAQTZF0RhBNAkiLldU3JAocftpNCrmJq64gSBhGTYFooIwICIMiyDbMMCwDzMwMz3dMz37Pu/veWusnu6eaWar6u7qes/z1NPVp6tO1fmcM+98v+d861QSlERABETAwQSSHHzvunUREAERgERMnUAERMDRBCRijm4+3bwIiIBETH1ABETA0QQkYo5uPt28CIiAREx9QAREwNEEJGKObj7dvAiIgERMfUAERMDRBCRijm4+3bwIiIBETH1ABETA0QQkYo5uPt28CIiAREx9QAREwNEEJGKObj7dvAiIgERMfUAERMDRBCRijm4+3bwIiIBjRay+vh61tXVqQREQAZcTcJyIUbyYDh7MxuzZy1zefKq+CIiA40SMTVZeXoF3352PX//6TuzefchoRVPc1KQiIALuIuAoETOFasuWXejf/wH89KfXYcKEd4wWq6uTa+murqvaikADAYeJWB1KSkrwzjtzceKJ/dClS3/07Dka6en7UF9fB1Pk1LgiIALuIeAYEWsQqHps2LAdAwc+iM6dr8HJJw9Gjx434okn3kRdXQ1qa2slZO7pu6qpCBgEHCNidBdLS0vx1luzDSusW7drccop1+KkkwYYY2O7du03RExupXq2CLiLgCNEzLTC1q3bimuvfQg//vE1hoBRxBqssZsMa6y2VtaYu7qvaisCcMbbjjjeVVZWhn/8Y6ZhhVG4KGDm1qXLAFx88R+RkXEQNTU1kDWmri0C7iEQ95aYaYVt2pSGESMm4LTThuDMM4eiW7cGIeMMJcfFzj9/BF588cPA2Jh7mlA1FQF3E4h7EaNVRTdx8+adePXVKXjuuTcxfPjj6Ny5P7p2HYTzzx+Op5/+O154YRImT/4ElZUVGuB3d59W7V1GwBEiVl5eDq/Xi/3792Hr1k14+eVJ+NGPrjYG9a+66k58+20q0tJ24NixYyguLjZcSoVbuKwnq7quJeAIEausrERBQQEOHz6M9PQdeO21D/DDHzaIWK9ed2H79u+wZ88e5ObmGjOYHBeTiLm2T6viLiPgCBGrrq42xMnj8eDIkcN4661/4gc/6AMO6Pfpcy8OHNiL7Oxs+P1+UPAUL2ZdL66rqUEtmVZWoj7sqQh+N/JragIXZF5dVZWRz8/wcwIHakcELCIQ9yLGenJcjEJGVzE/Pw/vvz8HP/hBb0PE+va9D7m52fD5fIaA8VhZYRb1DgCr7rwTk//t3/BGUhIOLlgQUvDemTPx0Q9/iHWPPRbIP7pyJWZ17453kpIw99xzkbd+feA37YiAHQQcIWIUJVpXHBvz+wswefI8dOpEEeuPfv3GwOvNC4yFKbzC2m5SV12NLwYNwltJSdj8wguoLikJXGDbxImYlJSElFtvRUV+fiA/OyUF0048EZ8PGIC62tpAvnZEwA4CjhAxVpziRFexuLgQkyfPDxExn89juJtyI+3oIsD6xx/H+0lJWD9+PCoLCoyLVPp8WHH77Xg9KQnLb74ZpUeOBC6e+fnnmHP22dg7fXogTzsiYBcBB4qYXyJmV2+IUO6uDz7A1P/8T0PMTBE7+OmnWHjFFYa4Lbn2Wvj37jXOrvL7seGJJ7C4d2/UVlVFKFHZImAdAYmYdSwTtqR9s2ZheteuWH7LLSg+fNioZ8qwYVhy3XWGqznv/PNxbNUqI//oqlX4/JprsHvy5ITloYrFFwGJWHy1R1zeTdby5Zh1xhn4YuBAlOXkYM9HH+GLAQNwePFipD7yCOaccw5y1qxBTWmpYYWtGjWqaT3q65ExbRo+69cPnm+/bfq7ckSgnQQkYu0E56bTfDt2YM6552LpkCHIWrYMnw8caIyPkcG3Tz2FqSecgAMLFmDvjBmGG8njzVRTVoYD8+bh8/79MevMMzH9Jz9B7tq15s/6FIEOE5CIdRhh4hdQdOAA5l94IRZddRU+uewypAwfDu+2bUbFNz//PKaffDLoXq4YMQLpkyaFACnav98QvKwvvwSP/ehHP0JuamrIMfoiAh0hIBHrCD0XnctwifeSkpB81lk49OmngZrTteR42ZROnRrixb5/kYt5QG1FBUqzsw1XkxMEjCuTiJl09GkFAYmYFRRdUAatsH8wVuz550EX0Ux7pkzBB0lJWNyrF/wZGWZ2k0+KGa00iVgTNMroIAGJWAcBuuV0z+bNoEtYnpcXUuWyo0fBKP2CXbtC8sO/1JSXS8TCoei7JQQkYpZgdFYhFKKtL72EVXfcga9GjsSWv/wFJd+HTjRbkzAXMXBMpPzAAY07ErFGFtqzloBEzFqecV8arSnOMn7cubMxxsVxLj4ixEeL9s+Zg3qbHhOiO7n7ww8Nd9KzcWPcc9INOoeARMw5bdXhOy0+dAgLLroI7yYlGZH2HMvixkeKmDfnF79A/pYttq08wceWOAEQPDHQ4UqpANcTkIi5pAsYgajjxxtiZYpX+CeF7OvRowPPR1qBhjOTG598EstuvhnTfvxjvM3VLc47D6vvvReH//UvKy6hMlxOQCLmkg5Q4fFgRrduIRZYuIjRImMcV0lmpmVUynNzwdUuaIVxZnPLSy/h22eewcY//9mYELDsQirItQQkYi5peopJuGg1953rgBUfOOASKqpmIhCQiCVCK7aiDrTE/nnyyS1aYpP//d+PP1PZimvpEBGIJgGJWDRpx/BaXMwwdexYY0ayOQuMeZyp5CNFFV5vDO9UlxaBthGQiLWNl3OPrq9H4e7dxgwkxYrjX6aYcZ95s3r0MFajsCvMwrnwdOfxTEAiFs+tY/W91deDa4Mt7tPHiNfibCQ3hj0suvJK7PrwQ9vixKyuisoTAZOARMwk4aJPrsK64U9/wrKbbsLSG24w1gTL/+47FxFQVROJgEQskVozQl32JyeDr15TEoFEJCARS8RWDaoTl4le2LNnUI52RSCxCEjEEqs9Q2pTVViIGaeeCs+mTSH5+iICiURAIpZIrRlWl7Vjx2LNmDFhufoqAolFQCKWWO0ZUhu+1IPWmJIIJDIBiVgit67qJgIuICARS8BG5uvTSrOyErBmqpIINCUgEWvKxNE5dB9ndu8OX1qao+uhmxeB1hKQiLWWlEOOW/foo+CmJAJuISARS6CW5rORtMI0mJ9AjaqqtEhAItYiIuccwKj8on37nHPDulMRsICARMwCiCpCBEQgdgQkYrFjb9mV6T5mfvaZZeWpIBFwEgGJmJNaK8K9MipfkfkR4Cg74QlIxBzexHwuks9HajDf4Q2p2283AYlYu9HFx4mLe/UCV6pQEgG3EpCIObzlGVahJAJuJiARc3Prq+4ikAAEJGIObcRDCxeCbzBSEgG3E5CIObAH5K1fr8F8B7abbtkeAhIxe7jaViqj8j+9/HLsnTHDtmuoYBFwEgGJmJNaCzBmIjkjqSQCItBAQCLmsJ7AcTCtFeawRtPt2kpAImYrXhUuAtYSOJZTi8VLKvD+tFLMmFuOzduqrb2AA0uTiDmk0bxbt0IxYQ5pLJtuc/O2Gjz7cjH6DvXiF1fk4ZK+HowcU4jkT8pRUlpv01Xjv1iJWPy3kXGHHMxXZL5DGsuG26RI3f2wH//d24MLfuMxPi/s5cGvrvTgN9flY8XqStS5VMckYjZ0OKuLpHhpMN9qqs4qb/aCMvzPoHxDvC7q44G5UdTOuzIPY5/04/CRWmdVyqK7lYhZBNKuYjiQrzXz7aLrnHKHjS4ICJcpYOYnLbKLenuQurHKORWy8E4lYhFg8p2Nq0aNwrTOnY2tYOfOwJGbnnkGs3v0wAdJSdifnBzIt2tHY2F2kXVOuX2H+ppYYaaI8fO/LsnD12slYnHdonV1daisrERxsR+TJ89Hp0690aVLf/TrNwY+nwelpaWora1FfX3HBwYoYAsuughHliwBBYtixU8mfm577TXwGOZT6JREwA4CR7Jr8c6UUqPo62/zRbTEDJeypwdr1kcWMf5Z0N38ak0lNmyuQmZWqOtZUVGPdd9WYc++mkBVamthHE8Lb+2GKpSVd/xvK1C4hTuyxCLApEgxmSJGQWMy87lPEaOg2ZWiYeXZde8qt/0EKF7PvlKMq4d68d7UUlRW1eOF14rx6/5Nx8RohXGg/7cjfdi5u1GAwq9OEfvn3HKcfVkezro0D89PLA45ZFtaNc64MBcj7y8I5JdX1GPQLV787OJcnHtFHrKP1SIeZUwiFmiy5nc+69vXEKtgd5JH8jtFLDy/+VLansvBfM5I8jEjJXcRoGBRvIpLGiVj/8FaDP19w7gYLS+KFz+5XXK1Bx/PLkNpWePxzRHjDOff3ilB9wtzMe6ZosAhtLCmzykzxG3o732BfO7k5NWCY24PPuGHBU5OSNlWfZGItUDSHPsKP4wWWurDD4dnW/Kdq7RO79oVfNBbKfEJ0IVbuqKyxYpS2G4e6cNl13hw/vdhFv1u9OL+x/1N3MNIhfE6p/0qFw9N8AcOoVD972OFOK9nHgYN8wby6+qAlNWVhgX4zfoqiViATDt3ojkmFnyLtLZojQUnWl8Ut2DX0vydLiB/M91PM78tn2vHjsXqe+5pyyk61oEEKF6PPl2EXkPy8XFyWatqwPGpx54pwtDbffjjmEJ8NLNlCyy4YA7+X3y1B2MebxSxTVurcXn/fAwe5kW/m7zIyW0YLysvr8cjf/aHHBtcVrzsyxI7TkvkrF1ruIxf3nhjyFHmoH9wJkWLYmdabh0RseJDh1CRnx9cvPYTjABduOtu9TVxG+2uJgfoGRx7yygfMvbXINdTh3HPFmH46AI890oxeg5qCJzlfXy3vdo4dlXQrCfH54qK65CXX4ecvDp48uvgL6oDrbZYJYnYcchTiGiJmTOTtLw4G7nrgw+anEXXkhaaeU5HRKxJ4cpICAK0vI7mhM4KRrti322rxpARXtz0Rx++3VKNz76swICbvfhyZSXe/rAUVwxsEDGOnz3wJz9eeSN04c05n5bj/vF+9Lkh33BrGYB7zyOF4GRErJJE7DjkTUGiQHGfllZzAhZchHlOe0QsZ80avbUoGGaC7Ae7jV990/LYl53VTt9Tg9vuKTDG1mZ/Um4I0Iv/1zBT+f60Mlzaz4O3J5fi4zlluP52H3wFoSYWn9ekBUkXlBYZZ1F/dZXHsChj9fymROw4PYaWlxnsSgusNTOR7RUxDubz1Wt8BZtS4hBgTFb/33oxc155yGxjrGpIF/LuhwuNcbiRYwpw+70FyPHUobq6HrMXlBvjZeOe9aP39flY0YzgXjk4H1NnlhnHsw50T3nsfeMKcez7sbRo100iZjHx9oqYXoBrcUPEsLh9BxvDYhgwynGkeEkcA/t/rxbj55fk4dpbfVi+qtEyZADsf12aZ8xGvvx6aBxZpPtfklKJ/xmYj49mlKGshRCPSGV0NF8i1lGCYee3R8RofTGkQi/ADYPpsK+m20jLK9ZjX5HQcTD+mb8WG7Ffr7/X8DSAeSwj+s+6LA+jHiw0s477yQH9W+9usOayjmpM7Liw+GOsQixavLGwA9ojYgxo5XphSs4lwFAH022MJ8srnCgDVmtqgarqetBKDE78jfmtia+mgN35YCH+cF+BEckfXE6092WJWUyckwDBM5oWF6/i4ohAsFjl++riym20ExMH9O8cWwiOqfFRpFgniZhFLWA+nkQBC97M8IzmLsNYMEXlN0cmvvPMZxs5KO7GxJCKvjd6sX5TFWpDJy9jgkMiFhPsDRdlVP66Rx+N4R3o0m0lwLgpPhTN2cZgS6yt5Tj1+Dc/KMUNt/uwdUfj2v5c8//VN0s0O9lSozplTKylepi/6wW4JglnfW5Pr3aleLGV0jNqjHX9z/l1nhHJz1U2aJHxOc7RYwtj5lrKEovB35BegBsD6O24JEMlGMz52tuhUevtKCohTuHSPPsP1oBcGG/G2Vhu3OdbmFozIWAHCImYHVRbUabGwloBKUaHFPrrjAeznTDbGCNEcXVZiVhcNYduJh4IMPRg3iJ3jnnFA/+23oNErK3EOnh8dkoKaisqOlhK5NO/+OILjBgxAklJScZ21llnYeLEiZFP0C+GS8Qlcfg8oJLzCEjEothmvrQ0IzLfrmV27r//fkO4unTpgjVr1sDr9YIiRkF76qmnolhTZ1zqYGat3EZnNNVx71Iidlw81v7Id0emv/eetYV+XxpFyrS+KGBmMvMpbEqhBDgo7dZQiVASzv4mEYtS++2dMcO2NfMpWqaADR48OKRGdCXN34LFLeQgl3yhaHG20Y3xXYncxBKxKLUuXUi73h8ZPAbGMbHgFPybW0WM4sUxL802BveMxNmXiDm8LQ8cOBCwtGhxhSdaZm63xOgyym0M7xmJ871pr4/Tujk1Yp/WV9G+fbZRTU5ODohUuCvJi5oCxk8O9Lsh0fJK/qTcDVVVHdnHnULBqSK2sGdP8B2SdiVzRjJYrJrb5yxloqdgt1Eiluit3Vg/iVgjC8v3KF4UMTvT5ZdfHrC2wse8ggf1KXaJnBigyvWt5DYmcis3XzeJWPNcOpwbrTXzg62u8JsOHg8LH/APP9aJ32l5cVNyNwGJmI3tz7cX2Z1MEQuPAwse8A//ze57srv8YLcx1m8PsruuKr9lAhKxlhnF9RGmiIUP6ge7ku+//35c16EtN8e16xUq0RZiiX+sRMzCNq6vr0dNVRUylywx3gnA79zsTKbLGCxinIWk9UWB45iZ0xMtr+AA1fC14Z1eP91/xwhIxDrGL3A2Z09rampw6Xm3oN/PB+D112djw4Y0VFZWGYLG3+0QNI51mdbYtm3bQDfSHOznp5PDKoLdRo19BbqadsIISMTCgLTnq2GB1dTgcHoGTjnl2pDtnHN+h5Ejn8PMmUtQWloWsNDac51I51DITOEyrS+nu5BLV1TKbYzU4MoPISARC8HRvi+0sioqKjBt2O9x2Tk3hIhYsKhdcMFtmDhxOnJzvaitrbVF0NpXg/g4iy92NVNZeX2IC2nm61MEwglIxMKJtOM7BamsrAy7V63C9o0bsXLlarz22mSMGvUMLrtsZBNRO+OMoRg37nVkZBw2xMwON7Md1YjZKabbyBdwBI99xeyGdGFHEZCIWdBcFLHy8nLk5eUhIyMDmzZtMtbz+vrrr8EtOflT3HHH0+jePdRKO/306/Hyy1NRXl7hWquM69drttGCTujiIiRiFjR+TmoqSvLyUFRUhKNHj2Lv3r3YsWMHNm/ejHXr1gUEbdGiL/DQQ3/FL385LMQ669v3PuzcuS/gYlpwS3FdRLC1xXc4Bn+P6xvXzcUlAYlYB5uFkfn/PPVUeHfuRFVVFUpKSowZwZycHGRmZhqClpaWZlhna9euNSyz5ctTMG7cxBDLjC7mG28kJ7RVZrqND4z3d5C6TheBRgISsUYW7dpbM2YMNk6YYIRPmGEWFDO6l8XFxSgoKEBubi4OHz6M3bt347vvvjOsM7qZs2YtwFVXjQ6xyn73uydQUOBPKPeScV1az6td3UsntYKARKwVkCId4tm0CbN79EB1SeN7CTlITzHjxrEyU9Doano8Hhw5ciQgZrTMVqxYibFjX8Jppw0JiNmQIY/A5ytMqEF/Ph4ktzFST1J+RwhIxDpAj28t8m7dGrEECpopatXV1UYYBq0zihkts507d2Ljxo1YvXo1pk6dEzJWRiHzegscKWSm26jlcCJ2Df1gIQGJmIUwj1eUKWaM6mdMGS0zjptxEmDLli2gVUb38txzbwmxyJwkZIzzktt4vF6g3+wgIBFrB1Wul388C+x4RZpiRsustLTUmAQ4dOiQMZuZmpraRMhGjHjKED26pjw3nlNxSb3W84rnBkrQe5OItaNhV99zD7h1JJljZrTKCgsLkZWVZbiXDMkIt8imTl1kjK3xnHhKfNns/z7mB8Mk2pu4iOGNf/Dhoj4eTPhLUaAYjqHd9XChkc/flUQgEgGJWCQyEfLz1q/HtM6dYcULcGlZmYP/dC+zs7MDQjZp0vSAW3n22Tdj795DoPUWD9bY9vRq3P1wYYeDVP8+qQQPT/Aj62itIVYUMiZ+v/3eAmzZUW0IG/O5ryQCzRGQiDVHJUJeXU2N8e5IK1+Aa7qXnMU0hYxxZXQtR49+JiBkN930uPFoUzy4lbSSrFgGmmJlJgoVhctM5m9/+Vsxrhycb2brUwSaEJCINUFy/Iyjq1aBYmZ1oqtoChnDMLZv346vvlqFSy75Q0DIJk2ai8rKSiN8w+rrH688uo0v/b0xjOR4x7bnt0VLKgxLjFZZeGJec/nhx+m7ewlIxOKk7YMtMo6RcbCfs5bTp88PxJBdfPEfUFBQGDW3kuIV7DbatRjh+x+XGSJGqys40Rrrf7NXrmQwFO03ISARa4Kk+YzslJTmf7Aw1xwj42A/FzNk+AXjyIYNezxgjSUnLzVmK6MxyM+HsxcvqYBd4mWio3jRnaRFFpw40B8sbBQ1jqOZEwE8h4P/Wmc/mJr79iVirWhzvvBjeteu4HOSdidTyBh+cezYMWOgf+rUuQERGzr0MWPszI5Bfgaprk6tsruKTco3ZyGDxYhjbsFjZDzJFDsKGZPphvJ8JfcSkIi10PYc/5p/wQW2vgA3/BZoZVGk/H6/4VZyNYwrr7wzIGTr1m2x1BozI+xvuN2H5asqw2/H9u8UK1pVZqI4Mc8c3DfzKWLh4Rbh55rH6tM9BCRiLbT1jtdft/0FuM3dAmchzTXK0tPT8de/vhcQsYcees1YLcOKmUo+z0ghiIbb2Fw9mWcKEcMoKFTNCVikcylqssQi0XFHvkSshXYuzcpC4e7dLRxl/c90K01r7ODBg0hNXRdYuqdXr7uN1TE4m9mesTFaXnz1Wbwk002kIHGQv7WJbiXDL4Ld0Naeq+MSh4BELI7bkpYWl73mM5ZcZNFctodrj2VnHzUstbaImOk2ciXVDZujP/ZlJWqKnQTMSqLOLUsiFqHtig8dsiQqP0LxrcqmQNHa8vl8xrLXXLPffPFISkqq8ewlha41iVH2ibIMtASsNS3unmMkYhHaenGvXtg7Y0aEX6OTTZeSq14wkp9xY88991ZAxN5+e5aRz995XHNp38HQoNxEWM+Ls5acBGiL29kcG+UlDgGJWDNtSfGiiMVDoqXFcAs+Vzl37mcBERs79pXAuFi4iDFIlUvicLYx+DVo8VCfjtwDB/7pQgYLmBkQ25Fyda6zCUjEwtqPsWAzu3eHLy0t7JfYfKVLyeBXLnGdkrI6IGKjRj1nLK4Y/hjSO1NKDfGK5WyjXaT4+BGtsOY2u66pcuOfgEQsrI0YFxaN6Pywy0b8ShGjUHE12AULlgZEbNy4vxnCRoHLy290Gwv9XBY7YnH6QQQSjoBELM6b1Bzc52NIycmN7uT48f+HpcszcecDObjzwcbVH+K8Oro9EbCcgEQsCCnXCou3ZIoYZyinTfskYIlddfVEDPhtOuYtLEB1dXwtlhhvDHU/iU1AIvZ9++6ePDkmkfktda9gEXv33bn42c9uNIRswp+nIDs7p82xYi1dT7+LgNMISMQA48HuGaeeCr6CLd4SRWz9pjKMvO8Qnnphh/ES3i+/XInU1G+NcTKOifEYJRFwKwGJGAC+AJdbvCUO0t/9cAGGjMjFnE/ykZmZbbyzks9S8pVvXHesvY8exVtddT8i0F4CEjEAHAuLxjI77WmkZV9VoKqq1giz4NvE+QgSl+jhQD8fSbLiIfD23JfOEYF4ISARi5eWAGCupBq+pheDWSlWDLVg4Cs3rnDBB8TlSsZRA+pWYkLA1SLGd0fyLd6xTul7aoxloBlhHylIlUJGwaKY8VEjfkrAYt1yun48EHCtiPGVa1ytNR4i8/mMYyTxCu8kFDNzC/9N30XAjQRcK2J8+e26Rx+NSZvTbRz/XBH4xmwlERCBjhFwpYhxIJ8hFdEezDfHvI7nNnasOXW2CLiPgCtFjOIVi+j8j5PLWu02uq8rqsYi0D4CrhSx9qFq+1m0vChcSiIgAvYRcJWI0QKLxnr5wW7j58tiP/tpX/dRySIQewKuEjEO5G+cMMFW6lwG54HxfrmNtlJW4SLQSMA1IsZQitk9eqC6pOHFq40IOr5Hy4urjiqJgAhEn4BrRIzLTe9PTraUcLDbGB5lb+mFVJgIiEBEAq4RMatXaz2YWZuwy0BH7C36QQTikIBrRMwK9rS8EuGNQVawUBkiEC8EEl7ErIgHC3Ybw1+DFi8NqfsQAbcSSGgR4yKHHY3M53sOFWHv1j8P1dsJBBJaxBb27AkuO93WxPEuM9F91NuDTBr6FIH4I5CwItaeNfNNt/G3f/Rp7Cv++qruSASaJZCwIla0b1+bovOffaVYbmOzXUSZIhDfBBJWxFqDPXgpnKM5tXIbWwNNx4hAnBFIOBEr2LWrxSV2TLfx0aeL4qw5dDsiIAJtJZBQIlZdWYlPL78cGdOmNcuBg/R3P1wot7FZOsoUAWcSSCgRS5s0qcUX4NIK02yjMzur7loEmiOQMCLmy8w01szP37IlUE/TbZy3qDyQpx0REIHEIpAwIlbk9eLIsmXGSzQYVU+38da7CrQkTmL1V9VGBJoQSBgR47sYzRfJ5vvq8NU3lU0qqwwREIHEI5AQIvavjzfivsd8yPVUG5ZY4jWTaiQCIhCJgKNF7NTuD6BXz1Xo23s75i30o6qqViIWqaWVLwI2EygsLIbHU2C83NnmS4UU72gR+/mZ4/HQ6Xdg/zdrEOxOhtRQX0RABGwlwJc5My1YsBJPPvkOli5NRU6ON2pi5mgRu+s/TsfjZ/w3/H4vKio4A6mX0draW1W4CDRLoOHvLjn5S5x33nCceGJf3Hbb01i+fD08Hh+qq2uaPcuqTAeKWCEmT56PTp1647ITLsU1PUciLW0XMjIOITPzGLKycpGVladNDNQHotIHco2/u8zMo3jzzVm44ILb8JOfDESXLv1x0kkDMDuWSIYAAAOOSURBVHz4BEPMCgr8tomZ40SsqKgQH344D506/QannDIYJ588GCec0A8nnNBXmxioD8SwD1C4unUbjFNOuTawUcgoarfc8qQhZqWlZairq7PKCDPKcZyI+f0FmDJlHk444Wr89KeD0a1bAzB+ahMD9YHY9YFg8QreZ5t07ToIp502BC++OAXl5da+i9UxIsbBw6qqKni9+diwYRNefPEtXH31XYbin3769dAmBuoDse0DFKlg8eL+qac2GBmDBj2AuXOXwuv1GX/HVlpjjhMxn8+HPXv24Ouvv8bChQsxf/58LF68GMuWfYkVK1KwYsUKbWKgPhClPrBy5QqkpPDvbjmeeupv+OUvbzGsLgoYXcs+fe7CG298hPXrN2Dfvn3weDwoLy8PBKZb4Vc6SsRqampQUlKCrKwsbNu2DatXr/4e4ApD1L755husWbNGmxioD0SxD/DvcPXqVXj22X8YInbSSQPRu/dovPLK21i8+F/G3+aWLVtw8OBBeL3egIhZIWAswzEixpulCUqX0u/3Izs7GxkZGUhPTze2Xbt2GRYarTRtYqA+EJ0+sHv3buPvLy1tO1566V0MH/44Xn11EhYuXITly5cjNTUV27dvNwQsLy/PMEL4N8y/ZTO+rKNi5igRY2X5fGRlZaUR3FpYWIj8/HxD3elmFhQUaBMD9YEo9QH+zdGyojhlZmZi+/YdWLMmFatXf4P169cb3hJdyKNHjxptUlZWhurqaksFzHGWmKnYVHGKGYFQ1Slq/NQmBuoD0e0D/NujONGAoHe0f/9+w0M6cOBAQLz4NA2P49+slRaYqQeOs8TMG+cnzVFtYqA+ELs+QFHiWHVFRQWKiooMy4zWGb0ku8XL1AJHi5hZCX2KgAjEjgD/idDKMr0i0zOiuNlheYXXVCIWTkTfRUAE2kzAtIYpWubGvGgkiVg0KOsaIiACthGQiNmGVgWLgAhEg4BELBqUdQ0REAHbCEjEbEOrgkVABKJBQCIWDcq6hgiIgG0EJGK2oVXBIiAC0SAgEYsGZV1DBETANgISMdvQqmAREIFoEJCIRYOyriECImAbAYmYbWhVsAiIQDQISMSiQVnXEAERsI2ARMw2tCpYBEQgGgQkYtGgrGuIgAjYRkAiZhtaFSwCIhANAhKxaFDWNURABGwjIBGzDa0KFgERiAYBiVg0KOsaIiACthGQiNmGVgWLgAhEg8D/B2TM2pdDicy4AAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common metric used to measure the similarity between two words is the cosine similarity, which measures the cosine of the angle between the two vectors that represent each of the words.\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "This similarity value is calculated by using this formula:\n",
    "\n",
    "$\\text{similarity} = \\cos(\\theta) = {\\mathbf{A} \\cdot \\mathbf{B} \\over \\|\\mathbf{A}\\|_2 \\|\\mathbf{B}\\|_2} $\n",
    "\n",
    "For the first part of the formula, we have to compute the dot product between vectors $\\mathbf{A}$ and $\\mathbf{B}$.\n",
    "\n",
    "$\\mathbf{A} \\cdot \\mathbf{B} = \\sum\\limits_{i=1}^{n}{A_i  B_i}$\n",
    "\n",
    "In the denominator, you have to calculate the Euclidean $(L^2)$ norm of each vector ($\\mathbf{A}$ and $\\mathbf{B}$) and multiply their results. The Euclidean norm is calculated using this formula:\n",
    "\n",
    "$\\|\\mathbf{A}\\|_2 = \\sqrt{\\sum\\limits_{i=1}^{n}{A_i^2}}$\n",
    "\n",
    "\n",
    "The resulting number of this cosine similarity formula lies with a range of $[-1,1]$ and it should be interpreted as a similarity score between two words. The higher the number, the higher will be the similarity between those words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4: \n",
    "\n",
    "L2  norm can be calculated with the linalg.norm function from numpy. We can check the result:\n",
    "\n",
    "np.linalg.norm([3, 4])\n",
    "\n",
    "Create a function that computes the cosine similarity between to vectors by using numpy. \n",
    "You can code the $L^2$ from scratch or you can use the [numpy implementation](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    \n",
    "    #Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9285714285714286\n",
      "0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity([1, 2, 3],[-1, -3, -2]))\n",
    "print(cosine_similarity([2.2, 1.8, 3.5, 0.9],[2.2, 1.8, 3.5, 0.9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5:\n",
    "\n",
    "Create a function that, given a word, it returns the top-n most similar words using cosine similarity.\n",
    "\n",
    ">*Hints:*\n",
    "\n",
    "> *The function should first compute all the pairwise similarities between the input word and all the words in the word embeddings dictionary*\n",
    "\n",
    "> *Then, it should order them based on their similarity scores from highest to lowest and return the top-n words as a result*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, n, words_vectors=embs_dict, sim_function= cosine_similarity):\n",
    "    \n",
    "    #Your code here\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if it works and how efficient it is! Print the 5 most similar words to 'cat', or any other word of your choice:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 1.0000000000000004), ('cats', 0.7321739024507731), ('kitten', 0.6452790939097054), ('dog', 0.6380517245741392), ('kittens', 0.6217938001100844)]\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:18.362392\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now() \n",
    "\n",
    "print(most_similar('cat', 5))\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works, but it takes some time to compute even though we are only using a shortened version of english word embeddings.\n",
    "If the embeddings vectors were normalized to have unit length, the cosine similarity formula between 2 vectors would be equivalent to their dot\n",
    "product. This would make the whole process more efficient, as the pairwise similarities between the embedding vector $v$ of a given word and the all words in the vocabulary, can be computed as: \n",
    "> $W\\cdot{v}.T$, where $W$, is the embedding matrix containing all the embedding vectors in our vocabulary.\n",
    "\n",
    "\n",
    "*About word embedding normalization:*\n",
    "\n",
    "\"*Most applications of word embeddings explore not the word vectors themselves, but relations between them to solve, for\n",
    "example, similarity and word relation tasks.For these tasks, it was found that using normalised word\n",
    "vectors improves performance. Word vector length is therefore typically ignored.\"*, ([Wilson and Schakel, 2015](https://arxiv.org/pdf/1510.02675.pdf))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6:  \n",
    "\n",
    "(i) Get the embeddings matrix $W$ and save it as a numpy array. (W should be of shape `N_WORDS` by `N_DIMENSIONS`)\n",
    "\n",
    "(ii) Normalise the vectors in $W$ to have unit length. You can do this using [sklearn normalize](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html). Then create a dictionary `norm_embs_dict` that maps the words to their respective normalized vectors.\n",
    "\n",
    "(iii) Finally, use `norm_embs_dict` to create a function that, given a word, it returns the top-n most similar words to the input word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(299999, 300)\n"
     ]
    }
   ],
   "source": [
    "# get the embeddings matrix W and save it as a numpy array:\n",
    "\n",
    "W= #Your code here\n",
    "\n",
    "\n",
    "print(W.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(299999, 300)\n",
      "0.9999999999999997\n"
     ]
    }
   ],
   "source": [
    "# normalize the embedding vectors in W:\n",
    "W_norm= #Your code here\n",
    "\n",
    "print(W_norm.shape)\n",
    "\n",
    "# create a dictionary that maps the words in our vocabulary to the normalized embedding vectors\n",
    "norm_embs_dict= #Your code here\n",
    "\n",
    "print(np.linalg.norm(W_norm[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_norm(word, n, words_vectors= norm_embs_dict):\n",
    "\n",
    "    #Your code here\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's print the 5 most similar words to 'cat', or any other word of your choice and time the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 1.0000000000000002), ('cats', 0.7321739024507734), ('kitten', 0.6452790939097055), ('dog', 0.6380517245741391), ('kittens', 0.6217938001100844)]\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:01.859662\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now() \n",
    "\n",
    "print(most_similar_norm('cat', 5))\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answering analogy questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7:\n",
    "\n",
    "The last exercise is really cool! One of the most intriguing properties of word embeddings is that we can perform meaningful algebraic operations over the vectors in order to get specific words.\n",
    "\n",
    "For example, if we perform an operation like this one:\n",
    "\n",
    "$embeddings['france'] - embeddings['paris'] + embeddings['london']$\n",
    "\n",
    "This results in a vector. If we find the word that its embedding is closest to that vector, we will easily realize that the word `england` will be near. Another famous example is:\n",
    "\n",
    "$embeddings['king'] - embeddings['man'] + embeddings['woman']$\n",
    "\n",
    "\n",
    "(i) Write a function that given 3 words: $w1$, $w2$, $w3$, it performs the following operation between their corresponding embedding vectors: $v1 + v2 - v3$ and returns the resulting vector $v= v1 + v2 - v3$.\n",
    "\n",
    "\n",
    "(ii) Write a function that given a vector $v$ , it returns the top-n words that their embedding vectors are closer to $v$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_operations(w1, w2, w3, words_vectors= norm_embs_dict):\n",
    "   \n",
    "    #Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_word_to_vector(v, n, words_vectors= norm_embs_dict):\n",
    "   \n",
    "    #Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the two functions to recreate word analogies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([('britain', 0.7665362964598285), ('london', 0.764604191378641), ('england', 0.7510193607016002)], [('tritos', -0.10602439568567051), ('fio', -0.09812177088592422), ('pona', -0.08587519213032217)])\n"
     ]
    }
   ],
   "source": [
    "vector=words_operations('france','paris', 'london')\n",
    "\n",
    "print(closest_word_to_vector(vector, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([('dog', 0.9414784776850844), ('dogs', 0.8906176511455945), ('cat', 0.8059432953607224)], [('sumaila', -0.08943437038867767), ('mosa', -0.08848868592895201), ('hewitson', -0.08324735208574367)])\n"
     ]
    }
   ],
   "source": [
    "vector=words_operations('cat', 'cats','dogs')\n",
    "\n",
    "print(closest_word_to_vector(vector, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([('king', 0.9371198707735), ('queen', 0.8282987753124363), ('princess', 0.7302147484298794)], [('pages/', -0.12721093991697652), ('t/c', -0.12508202828211618), ('dpv', -0.11553621972490596)])\n"
     ]
    }
   ],
   "source": [
    "vector=words_operations('king', 'man','woman')\n",
    "\n",
    "print(closest_word_to_vector(vector, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: depending on the quality of the embeddings and other factors, sometimes the input words can appear within the results. In order to avoid this you can modify the code in `closest_word_to_vector` function to exclude the input words from the output.*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
